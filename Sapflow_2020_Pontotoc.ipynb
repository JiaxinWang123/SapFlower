{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "13RXOoF15ivCxytGp2ATun-J7LvfE-TdW",
      "authorship_tag": "ABX9TyNc1jhdFZe2SX49/YQIECG/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaxinWang123/SapFlower/blob/main/Sapflow_2020_Pontotoc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PirMKZ8SH32V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkD0IxN1Gee9"
      },
      "outputs": [],
      "source": [
        "#### Training BiLSTM model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the file path for saving the CSV\n",
        "csv_file_path = \"/content/drive/MyDrive/Sapflow_epoch50/best_rmse_per_clone.csv\"\n",
        "\n",
        "# Define the Bi-LSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size=8, hidden_layer_size=80, output_size=1):  # Change input size to 8\n",
        "        super().__init__()\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size, bidirectional=True)  # Bi-directional LSTM\n",
        "\n",
        "        self.linear = nn.Linear(hidden_layer_size * 2, output_size)  # Double the hidden size for bidirectional LSTM\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        lstm_out, _ = self.lstm(input_seq.view(len(input_seq), 1, -1))\n",
        "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Load the data\n",
        "data_origin = pd.read_csv(\"/content/Pontotoc_2020_Sapflow_Cleaned_day153_280_removed_outliners.csv\")\n",
        "\n",
        "# Drop NaN values and select relevant columns\n",
        "selected_columns = ['Date', 'VPD', 'PAR_Den_Avg']  # Remove the clone column from selected columns\n",
        "subset_data = data_origin[selected_columns].copy()  # Make a copy of the DataFrame\n",
        "\n",
        "# Define a dictionary to store the best RMSE for each clone\n",
        "best_rmse_dict = {}\n",
        "\n",
        "# Iterate over all clones from column index 5 to the end of the dataset\n",
        "for clone_index in range(5, len(data_origin.columns)):\n",
        "    clone = data_origin.columns[clone_index]  # Get the clone name from column index\n",
        "    clone_data = data_origin[['Date', 'VPD', 'PAR_Den_Avg', clone]].copy()  # Make a copy of the DataFrame\n",
        "\n",
        "    # Drop NaN values for the current clone\n",
        "    clone_data.dropna(inplace=True)\n",
        "    clone_data = clone_data[clone_data[clone].values > 0.005]\n",
        "\n",
        "    # Rename the columns for clarity\n",
        "    clone_data.columns = ['Date', 'VPD', 'PAR_Den_Avg', clone]\n",
        "\n",
        "    # Prepare data for training\n",
        "    data = clone_data\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "    # Extract hour from the date\n",
        "    data['Hour'] = data['Date'].dt.hour\n",
        "    data['Date'] = data['Date'].dt.date\n",
        "\n",
        "    # Convert date to ordinal\n",
        "    data['Date'] = data['Date'].apply(lambda x: x.toordinal())\n",
        "\n",
        "    # Convert hour to one-hot encoding\n",
        "    data = pd.get_dummies(data, columns=['Hour'], prefix='Hour')\n",
        "\n",
        "    # Normalize the other columns\n",
        "    scaler = StandardScaler()\n",
        "    data['Date'] = scaler.fit_transform(data['Date'].values.reshape(-1,1))\n",
        "    data['PAR_Den_Avg'] = scaler.fit_transform(data['PAR_Den_Avg'].values.reshape(-1,1))\n",
        "    data['VPD'] = scaler.fit_transform(data['VPD'].values.reshape(-1,1))\n",
        "\n",
        "    # Define the input and target variables\n",
        "    X = data[['Date', 'VPD', 'PAR_Den_Avg'] + [col for col in data.columns if col.startswith('Hour_')]].values\n",
        "    y = data[clone].values\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
        "\n",
        "    # Convert the data into PyTorch tensors and move them to GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train).float().to(device)\n",
        "    y_train_tensor = torch.tensor(y_train).float().to(device)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test).float().to(device)\n",
        "    y_test_tensor = torch.tensor(y_test).float().to(device)\n",
        "\n",
        "    # Define the model, loss function, and optimizer and move the model to GPU\n",
        "    model = BiLSTM(input_size=X.shape[1], hidden_layer_size=80, output_size=1).to(device)\n",
        "    loss_function = nn.L1Loss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the model multiple times and select the best fit model based on RMSE\n",
        "    best_rmse = float('inf')\n",
        "    best_model = None\n",
        "\n",
        "    num_trainings = 5  # Number of times to train the model\n",
        "    for _ in range(num_trainings):\n",
        "\n",
        "        # Train the model\n",
        "        epochs = 50\n",
        "        for i in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            model.hidden = (torch.zeros(2, 1, model.hidden_layer_size).to(device),  # Bidirectional LSTM has 2 layers\n",
        "                            torch.zeros(2, 1, model.hidden_layer_size).to(device))\n",
        "\n",
        "            y_pred = model(X_train_tensor)\n",
        "\n",
        "            loss = loss_function(y_pred, y_train_tensor.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i % 5 == 1:\n",
        "                print(f'epoch: {i:3} loss: {loss.item():10.8f}')\n",
        "\n",
        "        # Evaluate the model\n",
        "        with torch.no_grad():\n",
        "            preds = model(X_test_tensor)\n",
        "            loss = loss_function(preds, y_test_tensor.unsqueeze(1))\n",
        "            print(f'Test loss: {loss.item()}')\n",
        "\n",
        "        # Calculate RMSE\n",
        "        rmse = mean_squared_error(y_test, preds.cpu().numpy(), squared=False)\n",
        "        print(f'RMSE: {rmse}')\n",
        "\n",
        "        # Check if current model is the best fit\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            best_model = model\n",
        "\n",
        "    # Store the best RMSE for this clone in the dictionary\n",
        "    best_rmse_dict[clone] = best_rmse\n",
        "\n",
        "    # Use the best model for predictions\n",
        "    with torch.no_grad():\n",
        "        preds = best_model(X_test_tensor)\n",
        "        loss = loss_function(preds, y_test_tensor.unsqueeze(1))\n",
        "        print(f'Best model test loss: {loss.item()}')\n",
        "\n",
        "    # Plot the results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(preds.cpu().numpy(), label='Predicted')\n",
        "    plt.plot(y_test, label='True')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate RMSE using the best model\n",
        "    rmse = mean_squared_error(y_test, preds.cpu().numpy(), squared=False)\n",
        "    print(f'Best model RMSE: {rmse}')\n",
        "\n",
        "    # Save the best model state\n",
        "    torch.save(best_model.state_dict(), f'/content/drive/MyDrive/Sapflow_epoch50/model_{clone}_model_state_dict.pt')\n",
        "\n",
        "    # Write clone and best RMSE to the CSV file\n",
        "    with open(csv_file_path, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Clone', 'Best_RMSE'])  # Write header\n",
        "        for clone, rmse in best_rmse_dict.items():\n",
        "            writer.writerow([clone, rmse])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import csv\n",
        "\n",
        "# Define the Bi-LSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size=8, hidden_layer_size=80, output_size=1):  # Change input size to 8\n",
        "        super().__init__()\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size, bidirectional=True)  # Bi-directional LSTM\n",
        "\n",
        "        self.linear = nn.Linear(hidden_layer_size * 2, output_size)  # Double the hidden size for bidirectional LSTM\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        lstm_out, _ = self.lstm(input_seq.view(len(input_seq), 1, -1))\n",
        "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Load the data\n",
        "data_origin = pd.read_csv(\"/content/Pontotoc_2020_Sapflow_Cleaned_day153_280_removed_outliners.csv\")\n",
        "\n",
        "# Load the best RMSE for each clone\n",
        "best_rmse_dict = {}\n",
        "with open(\"/content/drive/MyDrive/Sapflow_epoch50/best_rmse_per_clone.csv\", mode='r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        best_rmse_dict[row[0]] = float(row[1])\n",
        "\n",
        "# Iterate over all clones from column index 5 to the end of the dataset\n",
        "for clone_index in range(5, len(data_origin.columns)):\n",
        "    clone = data_origin.columns[clone_index]  # Get the clone name from column index\n",
        "\n",
        "    # Load the trained model for this clone\n",
        "    model = BiLSTM(input_size=27, hidden_layer_size=80, output_size=1)  # Adjust input size according to your data\n",
        "    model.load_state_dict(torch.load(f'/content/drive/MyDrive/Sapflow_epoch50/model_{clone}_model_state_dict.pt'))\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare your input data for this clone\n",
        "    # Drop NaN values and select relevant columns\n",
        "    selected_columns = ['Date', 'VPD', 'PAR_Den_Avg']  # Adjust according to your data\n",
        "    input_data_subset = data_origin[selected_columns].copy()\n",
        "\n",
        "    # Preprocess input data\n",
        "    input_data_subset['Date'] = pd.to_datetime(input_data_subset['Date'])\n",
        "    input_data_subset['Hour'] = input_data_subset['Date'].dt.hour\n",
        "    input_data_subset['Date'] = input_data_subset['Date'].dt.date\n",
        "    input_data_subset['Date'] = input_data_subset['Date'].apply(lambda x: x.toordinal())\n",
        "    input_data_subset = pd.get_dummies(input_data_subset, columns=['Hour'], prefix='Hour')\n",
        "\n",
        "    # Normalize the data\n",
        "    scaler = StandardScaler()\n",
        "    input_data_subset['Date'] = scaler.fit_transform(input_data_subset['Date'].values.reshape(-1, 1))\n",
        "    input_data_subset['PAR_Den_Avg'] = scaler.fit_transform(input_data_subset['PAR_Den_Avg'].values.reshape(-1, 1))\n",
        "    input_data_subset['VPD'] = scaler.fit_transform(input_data_subset['VPD'].values.reshape(-1, 1))\n",
        "\n",
        "    # Convert DataFrame to PyTorch tensor\n",
        "    input_tensor = torch.tensor(input_data_subset.values).float()\n",
        "\n",
        "    # Make predictions\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input_tensor)\n",
        "\n",
        "    # Convert predictions tensor to numpy array\n",
        "    predictions = predictions.numpy()\n",
        "\n",
        "    # Now 'predictions' contains the predicted values for this clone\n",
        "    print(f'Predictions for clone {clone}: {predictions}')\n"
      ],
      "metadata": {
        "id": "_1wo_loUNxb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import csv\n",
        "\n",
        "# Define the Bi-LSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size=8, hidden_layer_size=80, output_size=1):  # Change input size to 8\n",
        "        super().__init__()\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size, bidirectional=True)  # Bi-directional LSTM\n",
        "\n",
        "        self.linear = nn.Linear(hidden_layer_size * 2, output_size)  # Double the hidden size for bidirectional LSTM\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        lstm_out, _ = self.lstm(input_seq.view(len(input_seq), 1, -1))\n",
        "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Load the data\n",
        "data_origin = pd.read_csv(\"/content/Pontotoc_2020_Sapflow_Cleaned_day153_280_removed_outliners.csv\")\n",
        "\n",
        "# Load the best RMSE for each clone\n",
        "best_rmse_dict = {}\n",
        "with open(\"/content/drive/MyDrive/Sapflow_epoch50/best_rmse_per_clone.csv\", mode='r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        best_rmse_dict[row[0]] = float(row[1])\n",
        "\n",
        "# Create a dictionary to store predictions for each clone\n",
        "predictions_dict = {}\n",
        "\n",
        "# Iterate over all clones from column index 5 to the end of the dataset\n",
        "for clone_index in range(5, len(data_origin.columns)):\n",
        "    clone = data_origin.columns[clone_index]  # Get the clone name from column index\n",
        "\n",
        "    # Load the trained model for this clone\n",
        "    model = BiLSTM(input_size=27, hidden_layer_size=80, output_size=1)  # Adjust input size according to your data\n",
        "    model.load_state_dict(torch.load(f'/content/drive/MyDrive/Sapflow_epoch50/model_{clone}_model_state_dict.pt'))\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare your input data for this clone\n",
        "    # Drop NaN values and select relevant columns\n",
        "    selected_columns = ['Date', 'VPD', 'PAR_Den_Avg']  # Adjust according to your data\n",
        "    input_data_subset = data_origin[selected_columns].copy()\n",
        "\n",
        "    # Preprocess input data\n",
        "    input_data_subset['Date'] = pd.to_datetime(input_data_subset['Date'])\n",
        "    input_data_subset['Hour'] = input_data_subset['Date'].dt.hour\n",
        "    input_data_subset['Date'] = input_data_subset['Date'].dt.date\n",
        "    input_data_subset['Date'] = input_data_subset['Date'].apply(lambda x: x.toordinal())\n",
        "    input_data_subset = pd.get_dummies(input_data_subset, columns=['Hour'], prefix='Hour')\n",
        "\n",
        "    # Normalize the data\n",
        "    scaler = StandardScaler()\n",
        "    input_data_subset['Date'] = scaler.fit_transform(input_data_subset['Date'].values.reshape(-1, 1))\n",
        "    input_data_subset['PAR_Den_Avg'] = scaler.fit_transform(input_data_subset['PAR_Den_Avg'].values.reshape(-1, 1))\n",
        "    input_data_subset['VPD'] = scaler.fit_transform(input_data_subset['VPD'].values.reshape(-1, 1))\n",
        "\n",
        "    # Convert DataFrame to PyTorch tensor\n",
        "    input_tensor = torch.tensor(input_data_subset.values).float()\n",
        "\n",
        "    # Make predictions\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input_tensor)\n",
        "\n",
        "    # Convert predictions tensor to numpy array\n",
        "    predictions = predictions.numpy()\n",
        "\n",
        "    # Store predictions in the dictionary\n",
        "    predictions_dict[clone] = predictions.flatten()\n",
        "\n",
        "# Convert predictions dictionary to DataFrame\n",
        "predictions_df = pd.DataFrame(predictions_dict)\n",
        "\n",
        "# Now 'predictions_df' contains the predictions for each clone as columns in the DataFrame\n",
        "# You can save this DataFrame to a CSV file or perform further operations as needed\n",
        "predictions_df.to_csv(\"/content/drive/MyDrive/Sapflow_epoch50/predictions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "0h9eppxhdjRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the file path for saving the CSV\n",
        "csv_file_path = \"/content/drive/MyDrive/Sapflow_epoch50/best_rmse_per_clone.csv\"\n",
        "\n",
        "# Define the Bi-LSTM model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layer_size=80, output_size=1):\n",
        "        super().__init__()\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size, bidirectional=True)\n",
        "        self.linear = nn.Linear(hidden_layer_size * 2, output_size)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        lstm_out, _ = self.lstm(input_seq.view(len(input_seq), 1, -1))\n",
        "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
        "        return predictions\n",
        "\n",
        "# Load the data\n",
        "data_origin = pd.read_csv(\"/content/Pontotoc_2020_Sapflow_Cleaned_day153_280_removed_outliners.csv\")\n",
        "\n",
        "# Drop NaN values and select relevant columns\n",
        "selected_columns = ['Date', 'VPD', 'PAR_Den_Avg']  # Remove the clone column from selected columns\n",
        "subset_data = data_origin[selected_columns].copy()  # Make a copy of the DataFrame\n",
        "\n",
        "# Define a dictionary to store the best RMSE for each clone\n",
        "best_rmse_dict = {}\n",
        "\n",
        "# Define a dictionary to store trained models for each clone\n",
        "trained_models = {}\n",
        "\n",
        "# Iterate over all clones from column index 5 to the end of the dataset\n",
        "for clone_index in range(5, len(data_origin.columns)):\n",
        "    clone = data_origin.columns[clone_index]  # Get the clone name from column index\n",
        "    clone_data = data_origin[['Date', 'VPD', 'PAR_Den_Avg', clone]].copy()  # Make a copy of the DataFrame\n",
        "\n",
        "    # Drop NaN values for the current clone\n",
        "    clone_data.dropna(inplace=True)\n",
        "    clone_data = clone_data[clone_data[clone].values > 0.005]\n",
        "\n",
        "    # Rename the columns for clarity\n",
        "    clone_data.columns = ['Date', 'VPD', 'PAR_Den_Avg', clone]\n",
        "\n",
        "    # Prepare data for training\n",
        "    data = clone_data\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "    # Extract hour from the date\n",
        "    data['Hour'] = data['Date'].dt.hour\n",
        "    data['Date'] = data['Date'].dt.date\n",
        "\n",
        "    # Convert date to ordinal\n",
        "    data['Date'] = data['Date'].apply(lambda x: x.toordinal())\n",
        "\n",
        "    # Convert hour to one-hot encoding\n",
        "    data = pd.get_dummies(data, columns=['Hour'], prefix='Hour')\n",
        "\n",
        "    # Normalize the other columns\n",
        "    scaler = StandardScaler()\n",
        "    data['Date'] = scaler.fit_transform(data['Date'].values.reshape(-1,1))\n",
        "    data['PAR_Den_Avg'] = scaler.fit_transform(data['PAR_Den_Avg'].values.reshape(-1,1))\n",
        "    data['VPD'] = scaler.fit_transform(data['VPD'].values.reshape(-1,1))\n",
        "\n",
        "    # Define the input and target variables\n",
        "    X = data[['Date', 'VPD', 'PAR_Den_Avg'] + [col for col in data.columns if col.startswith('Hour_')]].values\n",
        "    y = data[clone].values\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
        "\n",
        "    # Convert the data into PyTorch tensors and move them to GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train).float().to(device)\n",
        "    y_train_tensor = torch.tensor(y_train).float().to(device)\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test).float().to(device)\n",
        "    y_test_tensor = torch.tensor(y_test).float().to(device)\n",
        "\n",
        "    # Define the model, loss function, and optimizer and move the model to GPU\n",
        "    model = BiLSTM(input_size=X.shape[1], hidden_layer_size=80, output_size=1).to(device)\n",
        "    loss_function = nn.L1Loss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the model multiple times and select the best fit model based on RMSE\n",
        "    best_rmse = float('inf')\n",
        "    best_model = None\n",
        "\n",
        "    num_trainings = 5  # Number of times to train the model\n",
        "    for _ in range(num_trainings):\n",
        "\n",
        "        # Train the model\n",
        "        epochs = 50\n",
        "        for i in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            model.hidden = (torch.zeros(2, 1, model.hidden_layer_size).to(device),\n",
        "                            torch.zeros(2, 1, model.hidden_layer_size).to(device))\n",
        "\n",
        "            y_pred = model(X_train_tensor)\n",
        "\n",
        "            loss = loss_function(y_pred, y_train_tensor.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i % 5 == 1:\n",
        "                print(f'epoch: {i:3} loss: {loss.item():10.8f}')\n",
        "\n",
        "        # Evaluate the model\n",
        "        with torch.no_grad():\n",
        "            preds = model(X_test_tensor)\n",
        "            loss = loss_function(preds, y_test_tensor.unsqueeze(1))\n",
        "            print(f'Test loss: {loss.item()}')\n",
        "\n",
        "        # Calculate RMSE\n",
        "        rmse = mean_squared_error(y_test, preds.cpu().numpy(), squared=False)\n",
        "        print(f'RMSE: {rmse}')\n",
        "\n",
        "        # Check if current model is the best fit\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            best_model = model\n",
        "\n",
        "    # Store the best RMSE for this clone in the dictionary\n",
        "    best_rmse_dict[clone] = best_rmse\n",
        "\n",
        "    # Save the best model state\n",
        "    model_path = f'/content/drive/MyDrive/Sapflow_epoch50/model_{clone}_model_state_dict.pt'\n",
        "    torch.save(best_model.state_dict(), model_path)\n",
        "    trained_models[clone] = model_path\n",
        "\n",
        "    # Write clone and best RMSE to the CSV file\n",
        "    with open(csv_file_path, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Clone', 'Best_RMSE'])  # Write header\n",
        "        for clone, rmse in best_rmse_dict.items():\n",
        "            writer.writerow([clone, rmse])\n",
        "\n",
        "# Now, let's make predictions for missing periods using the trained models\n",
        "# Load the data again to make predictions\n",
        "data = pd.read_csv(\"/content/Pontotoc_2020_Sapflow_Cleaned_day153_280_removed_outliners.csv\")\n",
        "\n",
        "# Identify clone columns\n",
        "clone_columns = [col for col in data.columns if col not in ['Date', 'VPD', 'PAR_Den_Avg']]\n",
        "\n",
        "# Identify missing periods for each clone\n",
        "missing_periods = {}\n",
        "for clone_column in clone_columns:\n",
        "    missing_periods[clone_column] = data[data[clone_column].isnull()]['Date'].tolist()\n",
        "\n",
        "# Preprocess the missing data and make predictions for each clone\n",
        "scaler = StandardScaler()\n",
        "for clone_column, missing_dates in missing_periods.items():\n",
        "    # Check if there are missing dates for the current clone column\n",
        "    if missing_dates:\n",
        "        # Prepare data for prediction\n",
        "        missing_data = pd.DataFrame({'Date': missing_dates})\n",
        "        missing_data['Date'] = pd.to_datetime(missing_data['Date'])  # Ensure 'Date' column is datetime\n",
        "        missing_data['Hour'] = missing_data['Date'].dt.hour\n",
        "        missing_data['Date'] = missing_data['Date'].dt.date\n",
        "        missing_data['Date'] = missing_data['Date'].apply(lambda x: x.toordinal())\n",
        "        missing_data = pd.get_dummies(missing_data, columns=['Hour'], prefix='Hour')\n",
        "\n",
        "        # Normalize the features\n",
        "        if 'PAR_Den_Avg' in missing_data.columns:\n",
        "            missing_data['PAR_Den_Avg'] = scaler.fit_transform(missing_data['PAR_Den_Avg'].values.reshape(-1,1))\n",
        "        if 'VPD' in missing_data.columns:\n",
        "            missing_data['VPD'] = scaler.fit_transform(missing_data['VPD'].values.reshape(-1,1))\n",
        "        print(f\"After normalization for {clone_column}:\\n{missing_data.head()}\")  # Debugging print statement\n",
        "\n",
        "        # Load the saved model\n",
        "        model_path = trained_models[clone_column]\n",
        "\n",
        "        # Load the model\n",
        "        model = BiLSTM(input_size=missing_data.shape[1] - 2, hidden_layer_size=80, output_size=1)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "        model.eval()\n",
        "\n",
        "        # Prepare input tensor for prediction\n",
        "        input_data = missing_data.drop(['Date', 'VPD', 'PAR_Den_Avg'], axis=1).values\n",
        "        input_tensor = torch.tensor(input_data).float()\n",
        "\n",
        "        # Make predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(input_tensor)\n",
        "\n",
        "        # Assign predicted values to the corresponding missing dates\n",
        "        data.loc[data['Date'].isin(missing_dates), clone_column] = predictions.numpy().flatten()\n",
        "\n",
        "# Save the predicted data to a new CSV file\n",
        "data.to_csv(\"predicted_data.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "CUfO_pjMQyQV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}